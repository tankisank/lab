{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a52d328207c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Create my_spark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    193\u001b[0m             )\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             self._do_init(\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;31m# preexec_fn not supported on Windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    856\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    859\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1309\u001b[0m             \u001b[1;31m# Start the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1311\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1312\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m                                          \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, isnan, when, count, udf, year, month, to_date, mean\n",
    "import pyspark.sql.functions as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create my_spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark)\n",
    "\n",
    "# Read File A\n",
    "spark_weather_dfs_a = spark.read \\\n",
    "    .option(\"header\", False) \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(path=f'data/weather/zurich_weather_a')\n",
    "\n",
    "spark_weather_dfs_a.printSchema()\n",
    "spark_weather_dfs_a.describe().show()\n",
    "\n",
    "# Read File B\n",
    "spark_weather_df_b = spark.read \\\n",
    "    .option(\"header\", False) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(path=f'data/weather/zurich_weather_b')\n",
    "\n",
    "spark_weather_df_b.printSchema()\n",
    "spark_weather_df_b.describe().show()\n",
    "\n",
    "# Drop unused Columns\n",
    "spark_weather_df_a = spark_weather_df_a.drop(\"_c0\")\n",
    "spark_weather_df_b = spark_weather_df_b.drop(\"_c0\")\n",
    "\n",
    "# Rename columns\n",
    "def rename_multiple_columns(df, columns):\n",
    "    if isinstance(columns, dict):\n",
    "        return df.select(*[F.col(col_name).alias(columns.get(col_name, col_name)) for col_name in df.columns])\n",
    "    else:\n",
    "        raise ValueError(\"columns need to be in dict format {'existing_name_a':'new_name_a', 'existing_name_b':'new_name_b'}\")\n",
    "\n",
    "dict_columns = {\"_c1\": \"date2\",\n",
    "                \"_c2\": \"avg_temp\",\n",
    "                \"_c3\": \"precip\",\n",
    "                \"_c4\": \"snow\",\n",
    "                \"_c5\": \"wind_dir\",\n",
    "                \"_c6\": \"wind_speed\",\n",
    "                \"_c7\": \"wind_power\",\n",
    "                \"_c8\": \"air_pressure\",\n",
    "                \"_c9\": \"sunny_hours\",}\n",
    "\n",
    "spark_weather_df_a_renamed = rename_multiple_columns(spark_weather_df_a, dict_columns)\n",
    "\n",
    "# Join the dataframes\n",
    "spark_weather_df = spark_weather_df_a_renamed.join(spark_weather_df_b_renamed, spark_weather_df_a_renamed.date2 == spark_weather_df_b_renamed.date, \"inner\")\n",
    "\n",
    "# Define a function for a quick data overview\n",
    "def quick_overview(dfs):\n",
    "    # Display the spark dataframe\n",
    "    display(\"FIRST RECORDS\")\n",
    "    display(dfs.limit(2).sort(col(\"date\"), ascending=True).toPandas())\n",
    "\n",
    "    # Count null values\n",
    "    display(\"COUNT NULL VALUES\")\n",
    "    display(dfs.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c, y in df.dtypes if y in [\"double\", \"float\"]]\n",
    "                      ).toPandas())\n",
    "\n",
    "    # Check for duplicates\n",
    "    duplicates = dfs.groupby(spark_weather_df.date) \\\n",
    "        .count() \\\n",
    "        .where('count > 1') \\\n",
    "        .limit(5).toPandas()\n",
    "    display(duplicates)\n",
    "\n",
    "    # Print schema\n",
    "    display(\"PRINT SCHEMA\")\n",
    "    display(dfs.printSchema())\n",
    "\n",
    "quick_overview(spark_weather_df)\n",
    "\n",
    "# Filter Null values\n",
    "spark_weather_df = spark_weather_df.filter(spark_weather_df.avg_temp.isNotNull())\n",
    "\n",
    "# Replace NA values with mean values\n",
    "avg = spark_weather_df.filter(spark_weather_df.avg_temp.isNotNull()) \\\n",
    "    .select(mean(col('min_temp')).alias('mean_min'),\n",
    "            mean(col('max_temp')).alias('mean_max'),\n",
    "            mean(col('wind_speed')).alias('mean_wind')).collect()\n",
    "\n",
    "mean_min = avg[0]['mean_min']\n",
    "mean_max = avg[0]['mean_max']\n",
    "mean_wind = avg[0]['mean_wind']\n",
    "\n",
    "spark_weather_df = spark_weather_df \\\n",
    "    .na.fill(value=mean_min, subset=[\"min_temp\"]) \\\n",
    "    .na.fill(value=mean_max, subset=[\"max_temp\"]) \\\n",
    "    .na.fill(value=mean_wind, subset=[\"wind_speed\"]) \\\n",
    "    .na.fill(value=0, subset=[\"snow\"])\n",
    "\n",
    "# Check for duplicates\n",
    "spark_weather_df.groupby(spark_weather_df.date) \\\n",
    "    .count() \\\n",
    "    .where('count > 1') \\\n",
    "    .limit(5).toPandas()\n",
    "\n",
    "# Remove duplicates and drop column date2, convert date to datatype \"date\", Sort the Data by Date\n",
    "spark_cleaned_df = spark_weather_df.dropDuplicates() \\\n",
    "    .drop(col(\"date2\")) \\\n",
    "    .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "    .orderBy(col(\"date\")) \\\n",
    "    .select(col(\"date\"), col(\"avg_temp\"), col(\"min_temp\"), col(\"max_temp\"), col(\"wind_speed\"), col(\"snow\"),\n",
    "            col(\"precip\"))  # Select columns\n",
    "\n",
    "quick_overview(spark_cleaned_df)\n",
    "\n",
    "# Create bucket column for temperature\n",
    "def binner(min_temp, max_temp):\n",
    "    if (min_temp is None) or (max_temp is None):\n",
    "        return \"unknown\"\n",
    "    else:\n",
    "        if min_temp < -10:\n",
    "            return \"freezing cold\"\n",
    "        elif min_temp < -5:\n",
    "            return \"very cold\"\n",
    "        elif min_temp < 0:\n",
    "            return \"cold\"\n",
    "        elif max_temp < 10:\n",
    "            return \"normal\"\n",
    "        elif max_temp < 20:\n",
    "            return \"warm\"\n",
    "        elif max_temp < 30:\n",
    "            return \"hot\"\n",
    "        elif max_temp >= 30:\n",
    "            return \"very hot\"\n",
    "        return \"normal\"\n",
    "\n",
    "udf_binner_temp = udf(binner, StringType())\n",
    "\n",
    "spark_cleaned_df = spark_cleaned_df.withColumn(\"temp_buckets\", udf_binner_temp(col(\"min_temp\"), col(\"max_temp\")))\n",
    "\n",
    "# Create new columns for bucket precipitation and for month and year\n",
    "udf_binner_precip = udf(lambda x: \"very rainy\" if x > 50 else (\"rainy\" if x > 0 else \"dry\"), StringType())\n",
    "\n",
    "spark_cleaned_df = spark_cleaned_df \\\n",
    "    .withColumn(\"precip_buckets\", udf_binner_precip(\"precip\")) \\\n",
    "    .withColumn(\"month\", month(spark_cleaned_df.date)) \\\n",
    "    .withColumn(\"year\", year(spark_cleaned_df.date))\n",
    "\n",
    "# Set the dimensions for the scatterplot\n",
    "fig, ax = plt.subplots(figsize=(28, 8))\n",
    "sns.scatterplot(hue=\"temp_buckets\", y=\"avg_temp\", x=\"date\", data=spark_cleaned_df.toPandas(), palette=\"Spectral_r\")\n",
    "\n",
    "# Plot formatting\n",
    "ax.tick_params(axis=\"x\", rotation=30, labelsize=10, length=0)\n",
    "\n",
    "# Title formatting\n",
    "mindate = str(spark_cleaned_df.agg({'date': 'min'}).collect()[0]['min(date)'])\n",
    "maxdate = str(spark_cleaned_df.agg({'date': 'max'}).collect()[0]['max(date)'])\n",
    "ax.set_title(\"average temperature in Zurich: \" + mindate + \" - \" + maxdate)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.show()\n",
    "\n",
    "# Data Analysis using PySpark.SQL\n",
    "\n",
    "# Register the dataset as a temp view, so we can use SQL\n",
    "spark_cleaned_df.createOrReplaceTempView(\"weather_data_temp_view\")\n",
    "\n",
    "# Perform SQL-like analysis\n",
    "events_over_years_df = spark.sql( \\\n",
    "    'SELECT year, month, mean(avg_temp), mean(max_temp), mean(min_temp) \\\n",
    "    FROM weather_data_temp_view \\\n",
    "    WHERE max_temp > 25 \\\n",
    "    GROUP BY month, year \\\n",
    "    ORDER BY year, month')\n",
    "\n",
    "print(events_over_years_df.limit(5).toPandas())\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "fig = sns.lineplot(y=\"mean(avg_temp)\", x=\"year\", data=events_over_years_df.toPandas(), color=\"orange\")\n",
    "fig = sns.lineplot(y=\"mean(max_temp)\", x=\"year\", data=events_over_years_df.toPandas(), color=\"red\")\n",
    "fig = sns.lineplot(y=\"mean(min_temp)\", x=\"year\", data=events_over_years_df.toPandas(), color=\"blue\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Scatter Plots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(30, 10))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.2)\n",
    "\n",
    "palette = sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True)\n",
    "sns.scatterplot(ax=axes[0], hue=\"snow\", size=\"snow\", y=\"max_temp\", x=\"min_temp\", data=spark_cleaned_df.toPandas(),\n",
    "                alpha=1.0, palette=palette)\n",
    "axes[0].legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "axes[0].set_title(\"min - max temperature separated by snow\")\n",
    "\n",
    "sns.scatterplot(ax=axes[1], hue=\"wind_speed\", size=\"wind_speed\", y=\"max_temp\", x=\"min_temp\", data=spark_cleaned_df.toPandas(),\n",
    "                alpha=1.0, palette='rocket_r')\n",
    "axes[1].legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "axes[1].set_title(\"min - max temperature separated by wind speed\")\n",
    "\n",
    "sns.scatterplot(ax=axes[2], hue=\"month\", y=\"max_temp\", x=\"min_temp\", data=spark_cleaned_df.toPandas(),\n",
    "                alpha=1.0, palette='Spectral_r', hue_norm=(1, 12), legend=\"full\")\n",
    "axes[2].legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "axes[2].set_title(\"min - max temperature separated by month\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
